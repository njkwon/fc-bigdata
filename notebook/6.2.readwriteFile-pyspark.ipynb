{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark context 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "from itertools import islice\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "bucket = \"fc-class\"     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text File Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = sc.textFile(\"s3://\"+ bucket +\"/ods/danji_master.csv\").map(lambda line:line.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF= spark.read.format(\"csv\").option(\"header\",\"true\").load(\"s3://\"+ bucket +\"/ods/danji_master.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV 파일 읽기 \n",
    "데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV To Json 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF.coalesce(1).write.mode(\"overwrite\").format('json').save(\"s3://\"+ bucket+\"/pyspark/class/danji_master.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json File Read\n",
    "Source 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json(\"s3://\"+ bucket+\"/pyspark/class/danji_master.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json File Read\n",
    "데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json To Parquet File Write\n",
    "Parquet 로 쓴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.coalesce(1).write.mode(\"overwrite\"). \\\n",
    "format('parquet').save(\"s3://\"+ bucket+\"/pyspark/class/danji_master.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet File Read\n",
    "Create Temp View Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq = spark.read.parquet(\"s3://\"+ bucket+\"/pyspark/class/danji_master.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq.createOrReplaceTempView(\"danji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 조회 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from danji\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select * from danji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB 읽기\n",
    "\n",
    "실습 6주차 참고하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcHostname = \"pipeline-instance-1.cp7ovqbewwso.ap-northeast-2.rds.amazonaws.com\"\n",
    "jdbcDatabase = \"datamart\"\n",
    "username=\"admin\"\n",
    "password=\"epdlxj!\"\n",
    "jdbcPort = 3306\n",
    "jdbcUrl = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}?characterEncoding=UTF-8\".format(jdbcHostname, jdbcPort, jdbcDatabase, username, password)\n",
    "\n",
    "sql=\"(select * from datamart.danji_master_data15 limit 10) a\"\n",
    "jdbcDF = spark.read.format(\"jdbc\").option(\"driver\",\"com.mysql.jdbc.Driver\" ).option(\"url\", jdbcUrl).option(\"dbtable\", sql).option(\"user\", username).option(\"password\", password).load()\n",
    "\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.cacheTable(\"danji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select dong, count(*)\n",
    "from danji\n",
    "group by dong\n",
    "order by count(*) desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# jdbcUrl = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}?characterEncoding=UTF-8\".format(jdbcHostname, jdbcPort, jdbcDatabase, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl = \"jdbc:mysql://{0}:{1}/{2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "\n",
    "connectionProperties = {   \n",
    "  \"user\" : username,\n",
    "  \"password\" : password,\n",
    "  \"driver\" : \"com.mysql.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"(select * from datamart.danji_master_data15 limit 10) a\"\n",
    "jdbcDF = spark.read.format(\"jdbc\").option(\"driver\",\"com.mysql.jdbc.Driver\" ).option(\"url\", jdbcUrl).option(\"dbtable\", sql).option(\"user\", username).option(\"password\", password).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

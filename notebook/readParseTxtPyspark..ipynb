{
  "metadata": {
    "name": "readParseTxtPyspark",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark \n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType\nimport json\n\n# Initialize a Spark session\n\nspark \u003d SparkSession.builder.appName(\"LogParsing\").getOrCreate()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\npath \u003d \"s3://fc-class/logs/*.gz\"\n\n#log_rdd \u003d spark.sparkContext.textFile(path)\n\ndf1 \u003d  spark.sparkContext.textFile(path)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef parse_raw_json(line):\n    pieces \u003d line.split(\"|\")\n    \n    adid \u003d pieces[1]\n    uuid \u003d pieces[2]\n    name \u003d pieces[3]\n    timestamp \u003d pieces[8]\n    gtm_times \u003d pieces[7]\n    \n    # JSON Parse\n    json_string \u003d pieces[9]\n    result \u003d json.loads(json_string)\n    \n    screen_name \u003d result.get(\"screen_name\", \"NULL\").replace(\"nil\", \"NULL\")\n    item_id \u003d result.get(\"item_id\", \"NULL\").replace(\"nil\", \"NULL\")\n    content_type \u003d result.get(\"content_type\", \"NULL\").replace(\"nil\", \"NULL\")\n    item_category \u003d result.get(\"item_category\", \"NULL\").replace(\"nil\", \"NULL\")\n    is_zb_agent \u003d result.get(\"is_zb_agent\", \"NULL\").replace(\"nil\", \"NULL\")\n    building_id \u003d result.get(\"building_id\", \"NULL\").replace(\"nil\", \"NULL\")\n    area_type_id \u003d result.get(\"area_type_id\", \"NULL\").replace(\"nil\", \"NULL\")\n    agent_id \u003d result.get(\"agent_id\", \"NULL\").replace(\"nil\", \"NULL\")\n    button_name \u003d result.get(\"button_name\", \"NULL\").replace(\"nil\", \"NULL\")\n    status \u003d result.get(\"status\", \"NULL\").replace(\"nil\", \"NULL\")\n    base_date \u003d timestamp[:10]\n    \n    return Row(base_date\u003dbase_date, adid\u003dadid, uuid\u003duuid, name\u003dname, timestamp\u003dtimestamp,\n               gtm_times\u003dgtm_times, screen_name\u003dscreen_name, item_id\u003ditem_id, content_type\u003dcontent_type,\n               item_category\u003ditem_category, is_zb_agent\u003dis_zb_agent, building_id\u003dbuilding_id,\n               area_type_id\u003darea_type_id, agent_id\u003dagent_id, status\u003dstatus, button_name\u003dbutton_name)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nlogs_df \u003d df1.map(parse_raw_json).toDF()\n\nlogs_df.createOrReplaceTempView(\"log_pyspark\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect count(*)\nfrom log_pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": "%%sql\n"
    }
  ]
}